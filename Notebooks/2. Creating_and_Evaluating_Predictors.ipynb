{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Creating and Evaluating Predictors - Target Time Series\n",
    "\n",
    "This notebook will build off of the earlier data processing that was performed in the validation sessions. If you have not completed that part yet, go back to `Validating_and_Importing_Target_Time_Series_Data.ipynb` and complete it first before resuming.\n",
    "\n",
    "At this point, you have target-time-series data loaded into Amazon Forecast inside a Dataset Group, this is what is required to use all of the models within Amazon Forecast. As an initial exploration, we will evaluate the results from ARIMA, Prophet, and DeepAR+. We could have also included ETS but have left it out for time constraints. Similarly, NPTS was left out as it specializes in spiky data or significant gaps that our dataset does not have.\n",
    "\n",
    "The very first thing to do is start with our imports, establish a connection to the Forecast service, and then restore our variables from before. The cells below will do that.\n",
    "\n",
    "Read more about available [algorithms](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-choosing-recipes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from time import sleep\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "import uuid\n",
    "from IPython.display import display, HTML\n",
    "import threading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region)\n",
    "forecast = session.client(service_name='forecast')\n",
    "forecast_query = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `%store -r` restores the offloaded variables to be used in the given notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Creating and Training Predictors\n",
    " \n",
    "Given that our data is hourly and we want to generate a forecast on the hour, Forecast limits us to a horizon of 500 of whatever the slice is. This means we will be able to predict about 20 days into the future.\n",
    "\n",
    "The cells below will define a few variables to be used with all of our models. Then there will be an API call to create each `Predictor` where they are based on ARIMA, Prophet, and DeepAR+ respectfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastHorizon = 480\n",
    "NumberOfBacktestWindows = 4\n",
    "BackTestWindowOffset = 480\n",
    "ForecastFrequency = \"H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_algorithmArn = 'arn:aws:forecast:::algorithm/ARIMA'\n",
    "prophet_algorithmArn = 'arn:aws:forecast:::algorithm/Prophet'\n",
    "deepAR_Plus_algorithmArn = 'arn:aws:forecast:::algorithm/Deep_AR_Plus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Specifics\n",
    "arima_predictorName= project+'_arima'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ARIMA:\n",
    "arima_create_predictor_response=forecast.create_predictor(\n",
    "    PredictorName=arima_predictorName, \n",
    "    AlgorithmArn=arima_algorithmArn,\n",
    "    ForecastHorizon=forecastHorizon,\n",
    "    PerformAutoML= False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters={\n",
    "        \"NumberOfBacktestWindows\": NumberOfBacktestWindows, \n",
    "        \"BackTestWindowOffset\": BackTestWindowOffset\n",
    "    }, \n",
    "    InputDataConfig={\n",
    "       \"DatasetGroupArn\": datasetGroupArn, \n",
    "       \"SupplementaryFeatures\": [ \n",
    "                     { \n",
    "                        \"Name\": \"holiday\",\n",
    "                        \"Value\": \"US\"\n",
    "                     }\n",
    "                  ]\n",
    "   },\n",
    "   FeaturizationConfig={\n",
    "       \"ForecastFrequency\": ForecastFrequency, \n",
    "       \"Featurizations\": [\n",
    "                          {\n",
    "                              \"AttributeName\": \"target_value\", \n",
    "                              \"FeaturizationPipeline\": [\n",
    "                              {\n",
    "                                  \"FeaturizationMethodName\": \"filling\", \n",
    "                                  \"FeaturizationMethodParameters\": \n",
    "                                  {\n",
    "                                      \"frontfill\": \"none\", \n",
    "                                      \"middlefill\": \"zero\", \n",
    "                                      \"backfill\": \"zero\"\n",
    "                                  }\n",
    "                              }\n",
    "                            ]\n",
    "                          }\n",
    "                        ]\n",
    "    }\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Specifics\n",
    "prophet_predictorName= project+'_prophet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Prophet:\n",
    "prophet_create_predictor_response=forecast.create_predictor(\n",
    "    PredictorName=prophet_predictorName, \n",
    "    AlgorithmArn=prophet_algorithmArn,\n",
    "    ForecastHorizon=forecastHorizon,\n",
    "    PerformAutoML= False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters={\n",
    "        \"NumberOfBacktestWindows\": NumberOfBacktestWindows, \n",
    "        \"BackTestWindowOffset\": BackTestWindowOffset\n",
    "    }, \n",
    "    InputDataConfig={\n",
    "        \"DatasetGroupArn\": datasetGroupArn, \n",
    "        \"SupplementaryFeatures\": [ \n",
    "             { \n",
    "                \"Name\": \"holiday\",\n",
    "                \"Value\": \"US\"\n",
    "             }\n",
    "        ]},\n",
    "        FeaturizationConfig={\n",
    "            \"ForecastFrequency\": ForecastFrequency, \n",
    "            \"Featurizations\": [\n",
    "                          {\n",
    "                              \"AttributeName\": \"target_value\", \n",
    "                              \"FeaturizationPipeline\": [\n",
    "                                  {\n",
    "                                      \"FeaturizationMethodName\": \"filling\", \n",
    "                                      \"FeaturizationMethodParameters\": \n",
    "                                        {\n",
    "                                            \"frontfill\": \"none\", \n",
    "                                            \"middlefill\": \"zero\", \n",
    "                                            \"backfill\": \"zero\"\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                        }\n",
    "            ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR+ Specifics\n",
    "deeparp_predictorName= project+'_deeparp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DeepAR+:\n",
    "deeparp_create_predictor_response=forecast.create_predictor(\n",
    "    PredictorName=deeparp_predictorName, \n",
    "    AlgorithmArn=deepAR_Plus_algorithmArn,\n",
    "    ForecastHorizon=forecastHorizon,\n",
    "    PerformAutoML= False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters={\n",
    "        \"NumberOfBacktestWindows\": NumberOfBacktestWindows, \n",
    "        \"BackTestWindowOffset\": BackTestWindowOffset\n",
    "    }, \n",
    "    InputDataConfig= {\n",
    "        \"DatasetGroupArn\": datasetGroupArn, \n",
    "        \"SupplementaryFeatures\": [ \n",
    "                     { \n",
    "                        \"Name\": \"holiday\",\n",
    "                        \"Value\": \"US\"\n",
    "                     }\n",
    "                  ]\n",
    "    },\n",
    "    FeaturizationConfig={\n",
    "        \"ForecastFrequency\": ForecastFrequency, \n",
    "        \"Featurizations\": [\n",
    "                          {\n",
    "                              \"AttributeName\": \"target_value\", \n",
    "                               \"FeaturizationPipeline\": [\n",
    "                                   {\n",
    "                                       \"FeaturizationMethodName\": \"filling\", \n",
    "                                       \"FeaturizationMethodParameters\": \n",
    "                                       {\n",
    "                                           \"frontfill\": \"none\", \n",
    "                                           \"middlefill\": \"zero\", \n",
    "                                           \"backfill\": \"zero\"\n",
    "                                       }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                    ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stay Away and Listen\n",
    "\n",
    "Please wait while all the training jobs are completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_function(name, response):\n",
    "    while True:\n",
    "        modelTrainStatus = forecast.describe_predictor(\n",
    "            PredictorArn=response[\"PredictorArn\"]\n",
    "        )['Status']\n",
    "        print(name, modelTrainStatus)\n",
    "        if modelTrainStatus != 'ACTIVE' and modelTrainStatus != 'CREATE_FAILED':\n",
    "            time.sleep(5)\n",
    "        else: \n",
    "            break\n",
    "\n",
    "responses={\n",
    "    \"arima\"  : arima_create_predictor_response, \n",
    "    \"prophet\": prophet_create_predictor_response, \n",
    "    \"deeparp\": deeparp_create_predictor_response\n",
    "}    \n",
    "threads = list()\n",
    "for (key, response) in responses.items():\n",
    "    thread = threading.Thread(target=thread_function,args=(key,response))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "    \n",
    "print(\"All threads finished execution\")\n",
    "\n",
    "# TODO: Add better threads synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Examining the Predictors\n",
    "\n",
    "> It can take about 20-30 minutes for the models to be trained. Just be patient or tell the funny story of how the ClearScale made something.\n",
    "\n",
    "Once each of the Predictors is in an `Active` state, you can get their metrics to better understand accuracy and behavior. These are computed based on the hold out periods we defined when building the Predictor. The metrics are meant to guide our decisions when we use a particular Predictor to generate a forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA\n",
    "\n",
    "ARIMA is one of the gold standards for time series forecasting:\n",
    "- This algorithm is not particularly sophisticated, but it is reliable and can help us understand a baseline of performance. \n",
    "- To note, it does **not** really understand seasonality very well, and it does **not** support any item metadata or related time-series information. \n",
    "- Due to that, we will explore it here but not after adding other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various [metrics](https://docs.aws.amazon.com/forecast/latest/dg/metrics.html) to check the model quality.\n",
    "\n",
    "We are going to stick with RMSE and WQL for the sake of simplicity:\n",
    "- RMSE (Root Mean Square Error) is the square of the error term, which is the difference between the actual target value, and the predicted (forecasted) value.\n",
    "- The RMSE metric favors a model whose individual errors are of consistent magnitude because large variations in error increase the RMSE.\n",
    "- Because of the squared error, a few poorly predicted values in an otherwise good forecast can increase the RMSE.\n",
    "\n",
    "[WQL](https://docs.aws.amazon.com/forecast/latest/dg/API_WeightedQuantileLoss.html) (Weightet Quantile Loss) is the difference between the predicted value and the actual value over the quantile, weighted (normalized) by dividing by the sum over all quantiles.\n",
    "\n",
    "Here, the quantiles show the percentage of the prediction horizon. E.g., 50% quantile of 500 days depicts the predictions *near* 250th day. \n",
    "\n",
    "> Please, be really aware, that these values may and would differ per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Metrics\n",
    "arima_arn = arima_create_predictor_response['PredictorArn']\n",
    "arima_metrics = forecast.get_accuracy_metrics(PredictorArn=arima_arn)\n",
    "\n",
    "arima_summary_metrics = arima_metrics[\"PredictorEvaluationResults\"][0][\"TestWindows\"][0]\n",
    "arima_display_data = {\n",
    "    \"RMSE\": [\n",
    "        arima_summary_metrics[\"Metrics\"][\"RMSE\"]\n",
    "    ],\n",
    "    \"10%\" : [\n",
    "        arima_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][2][\"LossValue\"]\n",
    "    ],\n",
    "    \"50%\" : [\n",
    "        arima_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][1][\"LossValue\"]\n",
    "    ],\n",
    "    \"90%\" : [\n",
    "        arima_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][0][\"LossValue\"]\n",
    "    ]\n",
    "}\n",
    "arima_display_data_frame = pd.DataFrame(arima_display_data, [\"Arima\"])\n",
    "\n",
    "display(arima_display_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again these particular values will help us evaluate the other predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet\n",
    "\n",
    "Prophet is especially useful for datasets that:\n",
    "- Contain an extended time period (months or years) of detailed historical observations (hourly, daily, or weekly)\n",
    "- Have multiple strong seasonalities\n",
    "- Include previously known important, but irregular, events\n",
    "- Have missing data points or large outliers\n",
    "- Have non-linear growth trends that are approaching a limit\n",
    "\n",
    "Prophet is an additive regression model with a piecewise linear or logistic growth curve trend. It includes a yearly seasonal component modeled using the Fourier series and a weekly seasonal component modeled using dummy variables.\n",
    "\n",
    "Same as ARIMA, now you should look at the metrics from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Metrics\n",
    "prophet_arn = prophet_create_predictor_response['PredictorArn']\n",
    "prophet_metrics = forecast.get_accuracy_metrics(PredictorArn=prophet_arn)\n",
    "\n",
    "prophet_summary_metrics = prophet_metrics[\"PredictorEvaluationResults\"][0][\"TestWindows\"][0]\n",
    "arima_vs_prophet_display_data = {\n",
    "    \"RMSE\": [\n",
    "        arima_summary_metrics[\"Metrics\"][\"RMSE\"],\n",
    "        prophet_summary_metrics[\"Metrics\"][\"RMSE\"]\n",
    "    ],\n",
    "    \"10%\" : [\n",
    "        arima_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][2][\"LossValue\"],\n",
    "        prophet_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][2][\"LossValue\"]\n",
    "    ],\n",
    "    \"50%\" : [\n",
    "        arima_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][1][\"LossValue\"],\n",
    "        prophet_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][1][\"LossValue\"]\n",
    "    ],\n",
    "    \"90%\" : [\n",
    "        arima_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][0][\"LossValue\"],\n",
    "        prophet_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][0][\"LossValue\"]\n",
    "    ]\n",
    "}\n",
    "arima_vs_prophet_display_data_frame = pd.DataFrame(arima_vs_prophet_display_data, [\"Arima\", \"Prophet\"])\n",
    "\n",
    "display(arima_vs_prophet_display_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this tells us is that when querying the 10% quantile, we see less of an error from Prophet, also in the 90%, but we see a bit worse performance in the 50% quantile. Next will be DeepAR.\n",
    "\n",
    "> To improve the results the related data is required for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+\n",
    "\n",
    "Amazon Forecast DeepAR+ is a supervised learning algorithm for forecasting scalar (one-dimensional) time series using recurrent neural networks (RNNs). Classical forecasting methods, such as autoregressive integrated moving average (ARIMA) or exponential smoothing (ETS), fit a single model to each individual time series and then use that model to extrapolate the time series into the future. In many applications, however, you have much similar time series across a set of cross-sectional units. These time-series groupings demand different products, server loads, and requests for web pages. In this case, it can be beneficial to train a single model jointly over all of the time series. DeepAR+ takes this approach. When your dataset contains hundreds of feature time series, the DeepAR+ algorithm outperforms the standard ARIMA and ETS methods. You can also use the trained model for generating forecasts for new time series that are similar to the ones it has been trained on.\n",
    "\n",
    "Same as Prophet and ARIMA, now you should look at the metrics from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR+ Metrics\n",
    "deeparp_arn = deeparp_create_predictor_response['PredictorArn']\n",
    "deeparp_metrics = forecast.get_accuracy_metrics(PredictorArn=deeparp_arn)\n",
    "\n",
    "deeparp_summary_metrics = deeparp_metrics[\"PredictorEvaluationResults\"][0][\"TestWindows\"][0]\n",
    "combined_display_data = {\n",
    "    \"RMSE\": [\n",
    "        arima_summary_metrics[\"Metrics\"][\"RMSE\"],\n",
    "        prophet_summary_metrics[\"Metrics\"][\"RMSE\"],\n",
    "        deeparp_summary_metrics[\"Metrics\"][\"RMSE\"]\n",
    "    ],\n",
    "    \"10%\" : [\n",
    "        arima_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][2][\"LossValue\"],\n",
    "        prophet_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][2][\"LossValue\"],\n",
    "        deeparp_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][2][\"LossValue\"]\n",
    "    ],\n",
    "    \"50%\" : [\n",
    "        arima_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][1][\"LossValue\"],\n",
    "        prophet_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][1][\"LossValue\"],\n",
    "        deeparp_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][1][\"LossValue\"]\n",
    "    ],\n",
    "    \"90%\" : [\n",
    "        arima_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][0][\"LossValue\"],\n",
    "        prophet_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][0][\"LossValue\"],\n",
    "        deeparp_summary_metrics[\"Metrics\"][\"WeightedQuantileLosses\"][0][\"LossValue\"]\n",
    "    ]\n",
    "}\n",
    "combined_display_data_frame = pd.DataFrame(combined_display_data, [\"Arima\", \"Prophet\", \"DeepAR+\"])\n",
    "\n",
    "display(combined_display_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now seeing major improvements in accuracy for the 10 and 50% quantiles with a bit worse performance on the 90%. To explore what this all looks like in a visual format we will now create a Forecast with each Predictor and then export it to s3 where we can download and explore the results.\n",
    "\n",
    "> To improve the results the related data is required for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Creating and Exporting Forecasts\n",
    "\n",
    "Inside Amazon Forecast, a Forecast is a rendered collection of all of your items, at every time interval, for all selected quantiles, for your given forecast horizon. This process takes the Predictor you just created and uses it to generate these inferences and to store them in a useful state. Once a Forecast exists within the service, you can query it and obtain a JSON response or use another API call to export it to a CSV that is stored in S3. \n",
    "\n",
    "This tutorial will focus on the S3 Export, as that is often an easy way to manually explore the data with many tools.\n",
    "\n",
    "These again will take some time to complete after you have executed the cells, so explore the console to see when they have completed.\n",
    "\n",
    "To do that, visit the Amazon Forecast Service page, then click your Dataset Group, and then click `Forecasts` on the left. They will say `Create in progress...` initially and then `Active` when ready for Export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA\n",
    "arima_forecastName = project+'_arima'\n",
    "arima_create_forecast_response=forecast.create_forecast(\n",
    "    ForecastName=arima_forecastName,\n",
    "    PredictorArn=arima_arn\n",
    ")\n",
    "arima_forecast_arn = arima_create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet\n",
    "prophet_forecastName = project+'_prophet'\n",
    "prophet_create_forecast_response=forecast.create_forecast(\n",
    "    ForecastName=prophet_forecastName,\n",
    "    PredictorArn=prophet_arn\n",
    ")\n",
    "prophet_forecast_arn = prophet_create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR+\n",
    "deeparp_forecastName = project+'_deeparp'\n",
    "deeparp_create_forecast_response=forecast.create_forecast(\n",
    "    ForecastName=deeparp_forecastName,\n",
    "    PredictorArn=deeparp_arn\n",
    ")\n",
    "deeparp_forecast_arn = deeparp_create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Await\n",
    "def thread_function(name, forecast_arn):\n",
    "    while True:\n",
    "        modelInferenceStatus = forecast.describe_forecast(\n",
    "            ForecastArn=forecast_arn\n",
    "        )['Status']\n",
    "        print(name, modelInferenceStatus)\n",
    "        if modelInferenceStatus != 'ACTIVE' and modelInferenceStatus != 'CREATE_FAILED':\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "forecast_arns={\n",
    "    \"arima\"  : arima_forecast_arn, \n",
    "    \"prophet\": prophet_forecast_arn, \n",
    "    \"deeparp\": deeparp_forecast_arn\n",
    "}    \n",
    "\n",
    "threads = list()\n",
    "for key, forecast_arn in forecast_arns.items():\n",
    "    thread = threading.Thread(target=thread_function,args=(key,forecast_arn))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "    \n",
    "print(\"All threads finished execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once they are `Active` you can start the export process. The code to do so is in the cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_path = \"s3://\" + bucket_name + \"/arima_1/\"\n",
    "arima_job_name = \"ArimaExport1\"\n",
    "arima_forecast_export_job = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=arima_job_name,\n",
    "    ForecastArn=arima_forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": arima_path,\n",
    "            \"RoleArn\": role_arn\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_path = \"s3://\" + bucket_name + \"/prophet_1/\"\n",
    "prophet_job_name = \"ProphetExport1\"\n",
    "prophet_forecast_export_job = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=prophet_job_name,\n",
    "    ForecastArn=prophet_forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": prophet_path,\n",
    "            \"RoleArn\": role_arn\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeparp_path = \"s3://\" + bucket_name + \"/deeparp_1/\"\n",
    "deeparp_job_name = \"DeepARPExport1\"\n",
    "deeparp_forecast_export_job = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=deeparp_job_name,\n",
    "    ForecastArn=deeparp_forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": deeparp_path,\n",
    "            \"RoleArn\": role_arn\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Await\n",
    "import threading\n",
    "\n",
    "def thread_function(name, forecast_export_job_arn):\n",
    "    while True:\n",
    "        exportStatus = forecast.describe_forecast_export_job(\n",
    "            ForecastExportJobArn=forecast_export_job_arn\n",
    "        )['Status']\n",
    "        print(name, exportStatus)\n",
    "        if exportStatus != 'ACTIVE' and exportStatus != 'CREATE_FAILED':\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "forecast_export_job_arns={\n",
    "    \"arima\"   : arima_forecast_export_job[\"ForecastExportJobArn\"], \n",
    "    \"prophet\" : prophet_forecast_export_job[\"ForecastExportJobArn\"], \n",
    "    \"deeparp\" : deeparp_forecast_export_job[\"ForecastExportJobArn\"]\n",
    "}    \n",
    "\n",
    "threads = list()\n",
    "for key, forecast_export_job_arn in forecast_export_job_arns.items():\n",
    "    thread = threading.Thread(target=thread_function,args=(key,forecast_export_job_arn))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "    \n",
    "print(\"All threads finished execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exporting process is another one of those items that will take several minutes to complete. Just poll for progress in the console. From the earlier page where you saw the status turn `Active` for a Forecast, click it, and you can see the progress of the export."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the Forecasts\n",
    "\n",
    "At this point, they are all exported into S3, but you need to obtain the results locally so we can explore them, the cells below will do that starting with ARIMA, then Prophet, and lastly DeepAR+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arima\n",
    "s3 = boto3.resource('s3')\n",
    "poc_bucket = boto3.resource('s3').Bucket(bucket_name)\n",
    "arima_filename = \"\"\n",
    "arima_files = list(poc_bucket.objects.filter(Prefix=\"arima_1\"))\n",
    "for file in arima_files:\n",
    "    # There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "    if \"csv\" in file.key:\n",
    "        arima_filename = file.key.split('/')[1]\n",
    "        s3.Bucket(bucket_name).download_file(file.key, data_dir+\"/\"+arima_filename)\n",
    "print(arima_filename)\n",
    "#Printed filename should not be empty!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet\n",
    "s3 = boto3.resource('s3')\n",
    "poc_bucket = boto3.resource('s3').Bucket(bucket_name)\n",
    "prophet_filename = \"\"\n",
    "prophet_files = list(poc_bucket.objects.filter(Prefix=\"prophet_1\"))\n",
    "for file in prophet_files:\n",
    "    # There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "    if \"csv\" in file.key:\n",
    "        prophet_filename = file.key.split('/')[1]\n",
    "        s3.Bucket(bucket_name).download_file(file.key, data_dir+\"/\"+prophet_filename)\n",
    "print(prophet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR+\n",
    "s3 = boto3.resource('s3')\n",
    "poc_bucket = boto3.resource('s3').Bucket(bucket_name)\n",
    "deeparp_filename = \"\"\n",
    "deeparp_files = list(poc_bucket.objects.filter(Prefix=\"deeparp_1\"))\n",
    "for file in deeparp_files:\n",
    "    # There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "    if \"csv\" in file.key:\n",
    "        deeparp_filename = file.key.split('/')[1]\n",
    "        s3.Bucket(bucket_name).download_file(file.key, data_dir+\"/\"+deeparp_filename)\n",
    "print(deeparp_filename)\n",
    "\n",
    "# Note: When the file name available it could be the case that data has not been downloaded yet. But at small data sizes it is unlikely to occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-Time DeepAR+\n",
    "\n",
    "forecast_query_client = boto3.client('forecastquery')\n",
    "\n",
    "real_time_forecast_response = forecast_query_client.query_forecast(\n",
    "    ForecastArn=deeparp_forecast_arn,\n",
    "    StartDate=\"2018-01-05T00:00:00\",\n",
    "    EndDate='2018-01-06T00:00:00',\n",
    "    Filters={\n",
    "        \"item_id\" : \"1\"\n",
    "    }\n",
    ")\n",
    "real_time_forecast_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically it is not a real-time prediction: the Forecast generates the forecasts set for the whole horizon and returns its subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### ARIMA Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Eval\n",
    "arima_predicts = pd.read_csv(data_dir+\"/\"+arima_filename)\n",
    "arima_predicts.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to datetime\n",
    "arima_predicts['date'] = pd.to_datetime(arima_predicts['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the timezone and make date the index\n",
    "arima_predicts['date'] = arima_predicts['date'].dt.tz_convert(None)\n",
    "arima_predicts.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (arima_predicts.index.min())\n",
    "print (arima_predicts.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see our prediction goes from Jan 01 to Jan 20 as expected, given our 480 interval forecast horizon. Also, we can see the cyclical nature of the predictions over the entire timeframe. \n",
    "\n",
    "Now we are going to create a data frame of the prediction values from this Forecast and the actual values.\n",
    "\n",
    "First, let us remove the column ID of the item before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts = arima_predicts[['p10', 'p50', 'p90']]\n",
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets slice validation to meet our needs\n",
    "validation_df = validation_time_series_df.copy()\n",
    "validation_df = validation_df.loc['2018-01-01':'2018-01-20']\n",
    "print (validation_df.index.min())\n",
    "print (validation_df.index.max())\n",
    "validation_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let us join the dataframes together\n",
    "arima_val_df = arima_predicts.join(validation_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "arima_val_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this particular plot is hard to see, let us pick a random day January 5th to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_val_df_jan_5 = arima_val_df.loc['2018-01-05':'2018-01-06']\n",
    "arima_val_df_jan_5.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have here the good match between predicted and actual value.\n",
    "\n",
    "Now this is pretty clear for p50 showcasing that it does a great job of predicting the volume. Let us now do this for Prophet and DeepAR+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet Validation\n",
    "\n",
    "We will speed up the prep work to just a few cells this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Eval\n",
    "prophet_predicts = pd.read_csv(data_dir+\"/\"+prophet_filename)\n",
    "prophet_predicts.sample()\n",
    "# Remove the timezone\n",
    "prophet_predicts['date'] = pd.to_datetime(prophet_predicts['date'])\n",
    "prophet_predicts['date'] = prophet_predicts['date'].dt.tz_convert(None)\n",
    "prophet_predicts.set_index('date', inplace=True)\n",
    "prophet_predicts = prophet_predicts[['p10', 'p50', 'p90']]\n",
    "# Finally let us join the dataframes together\n",
    "prophet_val_df = prophet_predicts.join(validation_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "prophet_val_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_val_df_jan_5 = prophet_val_df.loc['2018-01-05':'2018-01-06']\n",
    "prophet_val_df_jan_5.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR+ Eval\n",
    "deeparp_predicts = pd.read_csv(data_dir+\"/\"+deeparp_filename)\n",
    "deeparp_predicts.sample()\n",
    "# Remove the timezone\n",
    "deeparp_predicts['date'] = pd.to_datetime(deeparp_predicts['date'])\n",
    "deeparp_predicts['date'] = deeparp_predicts['date'].dt.tz_convert(None)\n",
    "deeparp_predicts.set_index('date', inplace=True)\n",
    "deeparp_predicts = deeparp_predicts[['p10', 'p50', 'p90']]\n",
    "# Finally let us join the dataframes together\n",
    "deeparp_val_df = deeparp_predicts.join(validation_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeparp_val_df_jan_5 = deeparp_val_df.loc['2018-01-05':'2018-01-06']\n",
    "deeparp_val_df_jan_5.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is particularly interesting here is that we were below the actual numbers for a good portion of the day, even with p90. We did see an excellent performance from Prophet, and the metrics indicate that DeepAR+ is objectively better here, so now we will add related time-series data to our project and see how the models behave then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Recap and Next Steps\n",
    "\n",
    "At this point, we can now see through the 3 plots below that DeepAR+ does an excellent job outside of the high ranges, and that perhaps adding related data could improve both Prophet and DeepAR+'s performance. The next thing to do is to move to the notebook for importing your related-time series data and then progress to the second Creating and Evaluating notebook that will explain how to leverage the related data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store prophet_summary_metrics\n",
    "%store deeparp_summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
